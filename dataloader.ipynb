{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folktables import ACSDataSource, ACSIncome, \\\n",
    "ACSPublicCoverage, ACSEmployment, ACSMobility, ACSTravelTime\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from features import FEATURE_SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, source_year, source_state, target_year, target_state, prediction_target, feature_sets_to_drop,\n",
    "                 n_target_train, n_source_train, horizon='1-Year', survey='person', random_state=137):\n",
    "        \"\"\"Dataloader class for loading data from folktables package for learning under distributional shifts.\n",
    "\n",
    "        Args:\n",
    "            source_year (int): The year of the source data. \n",
    "            source_state (int): The US state of the source data.\n",
    "            target_year (str): The year of the target data.\n",
    "            target_state (str): The US state of the target data.\n",
    "            prediction_target (str): The target variable for prediction. Must be one of the following: 'ACSIncome', \n",
    "                                    'ACSPublicCoverage', 'ACSEmployment', 'ACSMobility', 'ACSTravelTime'\n",
    "            features_to_drop (list[str]): The feature sets that should be removed from the data. If empty, the default feature set of folktables is used. \n",
    "                                        Can contain the following: 'WORK', 'SENSITIVE'.\n",
    "        \"\"\"\n",
    "        self.source_year = source_year\n",
    "        self.source_state = source_state\n",
    "        self.target_year = target_year\n",
    "        self.target_state = target_state\n",
    "\n",
    "        self.prediction_target = prediction_target\n",
    "        self.n_source_train = n_source_train\n",
    "        self.n_target_train = n_target_train\n",
    "\n",
    "        self.horizon = horizon\n",
    "        self.survey = survey\n",
    "\n",
    "        self.feature_sets_to_drop = feature_sets_to_drop\n",
    "\n",
    "        self.random_state = random_state\n",
    "\n",
    "\n",
    "    def load_acs_data(self):\n",
    "\n",
    "        # download target and source data for the respective state & year\n",
    "        source_data = ACSDataSource(survey_year=self.source_year,\n",
    "                                    horizon=self.horizon, survey=self.survey).get_data(states=[self.source_state], download=True)\n",
    "        target_data = ACSDataSource(survey_year=self.target_year,\n",
    "                                    horizon=self.horizon, survey=self.survey).get_data(states=[self.target_state], download=True)\n",
    "\n",
    "        # select prediction target\n",
    "        if self.prediction_target == 'ACSIncome':    \n",
    "            X_source, y_source, _  = ACSIncome.df_to_pandas(source_data)\n",
    "            X_target, y_target, _  = ACSIncome.df_to_pandas(target_data)\n",
    "        elif self.prediction_target == 'ACSPublicCoverage':\n",
    "            X_source, y_source, _  = ACSPublicCoverage.df_to_pandas(source_data)\n",
    "            X_target, y_target, _  = ACSPublicCoverage.df_to_pandas(target_data)\n",
    "        elif self.prediction_target == 'ACSEmployment':\n",
    "            X_source, y_source, _  = ACSEmployment.df_to_pandas(source_data)\n",
    "            X_target, y_target, _  = ACSEmployment.df_to_pandas(target_data)\n",
    "        elif self.prediction_target == 'ACSPublicCoverage':\n",
    "            X_source, y_source, _  = ACSPublicCoverage.df_to_pandas(source_data)\n",
    "            X_target, y_target, _  = ACSPublicCoverage.df_to_pandas(target_data)\n",
    "        elif self.prediction_target == 'ACSMobility':\n",
    "            X_source, y_source, _  = ACSMobility.df_to_pandas(source_data)\n",
    "            X_target, y_target, _  = ACSMobility.df_to_pandas(target_data)\n",
    "        elif self.prediction_target == 'ACSTravelTime':\n",
    "            X_source, y_source, _  = ACSTravelTime.df_to_pandas(source_data)\n",
    "            X_target, y_target, _  = ACSTravelTime.df_to_pandas(target_data)\n",
    "        \n",
    "\n",
    "        return X_source, y_source, X_target, y_target\n",
    "    \n",
    "    def train_test_split(self, X_source, y_source, X_target, y_target):\n",
    "        \n",
    "        # split\n",
    "        X_source_train, X_source_test, y_source_train, y_source_test = \\\n",
    "        train_test_split(X_source, y_source, train_size=self.n_source_train, random_state=self.random_state)\n",
    "        \n",
    "        X_target_train, X_target_test, y_target_train, y_target_test = \\\n",
    "        train_test_split(X_target, y_target, train_size=self.n_target_train, random_state=self.random_state)\n",
    "\n",
    "        # create training dataset out of source & target\n",
    "        X_train = np.concatenate((X_source_train, X_target_train), axis=0)\n",
    "        y_train = np.concatenate((y_source_train, y_target_train), axis=0)\n",
    "\n",
    "        # split remaining target data into validation & test set\n",
    "        X_target_test, X_target_val, y_target_test, y_target_val = \\\n",
    "        train_test_split(X_target_test, y_target_test, test_size = 0.5, random_state=self.random_state)\n",
    "\n",
    "        return X_train, y_train, X_source_test, y_source_test, X_target_val, y_target_val, X_target_test, y_target_test\n",
    "\n",
    "    def drop_feature_sets(self,  X_source, X_target):\n",
    "        \n",
    "        columns_to_drop = \\\n",
    "        [item for sublist in [FEATURE_SETS[key] for key in self.feature_sets_to_drop] for item in sublist]\n",
    "        \n",
    "        filter_source = X_source.filter(columns_to_drop)\n",
    "        filter_target = X_target.filter(columns_to_drop)\n",
    "\n",
    "        X_source = X_source.drop(filter_source, axis=1)\n",
    "        X_target = X_target.drop(filter_target, axis=1)\n",
    "        \n",
    "        return X_source, X_target\n",
    "\n",
    "    def load_data(self):\n",
    "\n",
    "        # load ACS source & target data\n",
    "        X_source, y_source, X_target, y_target = self.load_acs_data()\n",
    "\n",
    "        # drop feature sets from default folktables features\n",
    "        if self.feature_sets_to_drop:\n",
    "             X_source, X_target = self.drop_feature_sets(X_source, X_target)\n",
    "\n",
    "        X_source, y_source, X_target, y_target = X_source.to_numpy(), y_source.to_numpy(), X_target.to_numpy(), y_target.to_numpy()\n",
    "\n",
    "        # split data into train, validation and test set\n",
    "        X_train, y_train, X_source_test, y_source_test, X_target_val, y_target_val, X_target_test, y_target_test = \\\n",
    "        self.train_test_split(X_source, y_source, X_target, y_target)\n",
    "\n",
    "        return X_train, y_train, X_source_test, y_source_test, X_target_val, y_target_val, X_target_test, y_target_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(source_state='FL', source_year='2014', target_state='CA', target_year='2014',\n",
    "prediction_target='ACSPublicCoverage', feature_sets_to_drop=[], n_source_train=50, n_target_train=10)\n",
    "\n",
    "X_train, y_train, X_source_test, y_source_test, X_source_val, y_source_val, X_target_test, y_target_test = loader.load_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6d537c67b1bf0617c73a31746a4fe04186b3f58f616a7703da327a943339fd0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
